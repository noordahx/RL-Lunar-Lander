{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import base64, io, time, gym\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import glob\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env: gym.Env = gym.make('LunarLander-v2')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnn(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super(Qnn, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE: int = 100000\n",
    "BATCH_SIZE: int = 64\n",
    "GAMMA: float = 0.99\n",
    "TAU: float = 1e-3\n",
    "LR: float = 5e-4\n",
    "UPDATE_EVERY: int = 4\n",
    "STATE_SIZE: int = env.observation_space.shape[0]\n",
    "ACTION_SIZE: int = env.action_space.n\n",
    "SEED: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, action_size: int, buffer_size: int, batch_size: int, seed: int):\n",
    "        self.action_size: int = action_size\n",
    "        self.memory: deque = deque(maxlen=buffer_size)\n",
    "        self.batch_size: int = batch_size\n",
    "        self.experience: namedtuple = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed: int = random.seed(seed)\n",
    "\n",
    "    def add(self, state: Tuple, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
    "        e: namedtuple = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self) -> Tuple:\n",
    "        experiences: Tuple = random.sample(self.memory, k=self.batch_size)\n",
    "        states: torch.Tensor = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions: torch.Tensor = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards: torch.Tensor = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states: torch.Tensor = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones: torch.Tensor = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int):\n",
    "        self.state_size: int = state_size\n",
    "        self.action_size: int = action_size\n",
    "        self.seed: int = random.seed(seed)\n",
    "        self.qnn_local: Qnn = Qnn(state_size, action_size).to(device)\n",
    "        self.qnn_target: Qnn = Qnn(state_size, action_size).to(device)\n",
    "        self.optimizer: optim = optim.Adam(self.qnn_local.parameters(), lr=LR)\n",
    "        self.memory: ReplayBuffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step: int = 0\n",
    "    \n",
    "    def step(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences: Tuple = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state: np.ndarray, eps: float = 0.) -> int:\n",
    "        state: torch.Tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnn_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values: torch.Tensor = self.qnn_local(state)\n",
    "        self.qnn_local.train()\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences: Tuple, gamma: float):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        q_target_next: torch.Tensor = self.qnn_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_target: torch.Tensor = rewards + (gamma * q_target_next * (1 - dones))\n",
    "        q_expected: torch.Tensor = self.qnn_local(states).gather(1, actions)\n",
    "        loss: torch.Tensor = F.mse_loss(q_expected, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.qnn_local, self.qnn_target, TAU)\n",
    "    \n",
    "    def soft_update(self, local_model: nn.Module, target_model: nn.Module, tau: float):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurda/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -186.65\n",
      "Episode 200\tAverage Score: -128.26\n",
      "Episode 300\tAverage Score: -47.714\n",
      "Episode 400\tAverage Score: -25.12\n",
      "Episode 500\tAverage Score: 100.12\n",
      "Episode 600\tAverage Score: 173.28\n",
      "Episode 700\tAverage Score: 200.49\n",
      "\n",
      "Environment solved in 600 episodes!\tAverage Score: 200.49\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "from typing import List, Tuple, Deque\n",
    "\n",
    "\n",
    "def DQN(n_episodes: int = 2000, max_t: int = 1000, eps_start: float = 1.0, eps_end: float = 0.01, eps_decay: float = 0.995):\n",
    "    scores: List[float] = []\n",
    "    scores_window: deque = deque(maxlen=100)\n",
    "    eps: float = eps_start\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state: np.ndarray = env.reset()[0]\n",
    "        score: float = 0\n",
    "        for t in range(max_t):\n",
    "            action: int = agent.act(state, eps)\n",
    "            next_state, reward, terminate, truncated, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, terminate)\n",
    "            state: np.ndarray = next_state\n",
    "            score += reward\n",
    "            if terminate or truncated:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(\n",
    "            i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(\n",
    "                i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 200:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n",
    "                i_episode - 100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnn_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "\n",
    "agent: Agent = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, seed=0)\n",
    "scores: List[float] = DQN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
